{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks\n",
    "\n",
    "RRNs are designed to learn from sequential data. \n",
    "It performs the same task for every element of a sequence.\n",
    "\n",
    "\n",
    "RNNs are used in language modeling, ie predicting what word come next for a given sequence of words (sentence).\n",
    "\n",
    "RNN should work on sequences of arbitrary lengths of input elements.\n",
    "\n",
    "So that, RNNs model have connections backward. It has a memory of current state that has been calculated so far.\n",
    "\n",
    "\n",
    "We can consider RNNs create a new layer of neurons for each input, but each layer shares a common set of weights. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let consider;\n",
    "\n",
    "- $x_t$ as the input at time $t$\n",
    "- $h_t$ as the hidden state at time $t$\n",
    "- $o_t$ as the output at time $t$\n",
    "\n",
    "Imagine we are going to train a sequence of words, where each word is in one-hot encoded form. \n",
    "\n",
    "Then $x_t \\in {\\rm I\\!R}^{v x 1} $, where $v$ is the size of the vocabulary\n",
    "\n",
    "The hidden layer can be defined with any length of choice, $n$.\n",
    "Therefore, weight metric connecting input layar to hidden layer will be $W_{xh} \\in {\\rm I\\!R}^{n x v}$\n",
    "\n",
    "The hidden layer $h_t$ is connected to next hidden layer $h_{t+1}$ with weight metric $W_{hh} \\in {\\rm I\\!R}^{n x n}$\n",
    "\n",
    "$h_t$ is represented as;\n",
    "\n",
    "$\n",
    "h_t = f(W_{hh} h_{t-1} + W_{xh} x_t) \\text{, where $f$ is a non-linear activation funtion, e.g., tanh or ReLU}\n",
    "$\n",
    "\n",
    "The connection from hidden layer to the output layer is just like connection from a fully connected layer to the output layer as in CNN.\n",
    "\n",
    "The predicted output vector $o_t \\in {\\rm I\\!R}^{v x 1} $ is connected from hiden layer with weight metric $W_{ho} \\in {\\rm I\\!R}^{n x v}$ \n",
    "\n",
    "$o_t$ can be represented as; \n",
    "\n",
    "$\n",
    "o_t = g(W_{ho} h_{t} + b_o) \\text{, where $g$ is softmax function and $b \\in {\\rm I\\!R}^{n x 1}$ is a bias vector }\n",
    "$\n",
    "\n",
    "\n",
    "The cost at each time step is the negative log cost over all the vocabulary size $v$.\n",
    "\n",
    "$\n",
    "cost(y_t, o_t) = cross-entropy(y_t, o_t) = - \\sum_{i=1}^{v}y_t log o_{t}\n",
    "$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Recurrent Neural Networks\n",
    "\n",
    "LSTM is a special version of RNN that can learn past information in a sequance.\n",
    "\n",
    "Standard RNN cannot learn correlation between distance element in a sequance.\n",
    "\n",
    "are special versions\n",
    "of RNNs that can learn distant dependencies between words in a sentence. \n",
    "\n",
    "At sequance step $t$, with the input $x_i$ and the previous step's hidden states $h_{t-1}$;\n",
    "- decide what to forget from cell state $C_{t-1}$ throught the `forget layer`, which outputs $f_t = g(W_f x_t + W_{fh} h_{t-1})$, where $g$ is sigmoid activiation.\n",
    "- next `input layer` decide which new information should be updated in the cell state, which output $i_t=g(W_i x_t + W_{ih} h_{t-1})$, where $g$ is sigmoid activiation.\n",
    "- the, a candidate set of new values, $\\overline{C}_t$ is created for the cell state using $\\overline{C}_t=tanh(W_c x_t + W_{ch} h_{t-1})$\n",
    "- then the cell state $C_t$ is updated as $C_t = f_t * C_{t-1} + i_t * \\overline{C}_t$, where `*` represent element wise multiplication.\n",
    "- the `output layer` determine which to output from cell state as it contains a lot of information, $o_t = g(W_{o} x_t + W_{oh} h_{t-1})$, where $g$ is sigmoid activiation.\n",
    "- finally the hidden state $h_t$ is updated with cell state as $h_t = o_t * tanh(C_t)$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next-Word Prediction with LSTM RNN\n",
    "\n",
    "The following passage from `Alice in Wonderland` is used to predict the next word using LSTM.\n",
    "(The example and code is taken from the book, Pro Deep Learning with TensorFlow by Santanu Pattanayak)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"' You can't think how glad I am to see you again , you dear old thing ! ' said the Duchess , as she tucked her arm affectionately into Alice's , and they walked off together . Alice was very glad to find her in such a pleasant temper , and thought to herself that perhaps it was only the pepper that had made her so savage when they met in the kitchen . ' When I'm a Duchess , ' she said to herself , ( not in a very hopeful tone though ) , ' I won't have any pepper in my kitchen at all . Soup does very well without \\xe2\\x80\\x94 Maybe it's always pepper that makes people hot-tempered , ' she went on , very much pleased at having found out a new kind of rule , ' and vinegar that makes them sour \\xe2\\x80\\x94 and camomile that makes them bitter \\xe2\\x80\\x94 and \\xe2\\x80\\x94 and barley-sugar and such things that make children sweet-tempered . I only wish people knew that : then they wouldn't be so stingy about it , you know \\xe2\\x80\\x94 'She had quite forgotten the Duchess by this time , and was a little startled when she heard her voice close to her ear. ' You're thinking about something , my dear , and that makes you forget to talk . I can't tell you just now what the moral of that is , but I shall remember it in a bit . ' ' Perhaps it hasn't one , ' Alice ventured to remark . ' Tut , tut , child ! ' said the Duchess . ' Everything's got a moral , if only you can find it . ' And she squeezed herself up closer to Alice's side as she spoke . Alice did not much like keeping so close to her : first , because the Duchess was very ugly ; and secondly , because she was exactly the right height to rest her chin upon Alice's shoulder , and it was an uncomfortably sharp chin . However, she did not like to be rude , so she bore it as well as she could .\\n\"]\n"
     ]
    }
   ],
   "source": [
    "def print_file(fname):\n",
    "    with open(fname) as f:\n",
    "        print(f.readlines())\n",
    "        \n",
    "print_file('AliceInWonderland.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "import random\n",
    "import collections\n",
    "import time\n",
    "\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A sequence of 3 words is used as the input, and subsequent word is taken as the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "training_iters = 50000\n",
    "display_step = 500\n",
    "n_input = 3\n",
    "\n",
    "n_hidden = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read and process the input file\n",
    "def read_data(fname):\n",
    "    with open(fname) as f:\n",
    "        data = f.readlines()\n",
    "    data = [x.strip() for x in data]\n",
    "    data = [data[i].lower().split() for i in range(len(data))]\n",
    "    data = np.array(data)\n",
    "    data = np.reshape(data, [-1, ])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to build dictionary and reverse dictionary of words.\n",
    "def build_dataset(train_data):\n",
    "    count = collections.Counter(train_data).most_common()\n",
    "    dictionary = dict()\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary)\n",
    "    reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    return dictionary, reverse_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = 'AliceInWonderland.txt'\n",
    "train_data = read_data(train_file)\n",
    "dictionary, reverse_dictionary = build_dataset(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Place holder for Mini-batch input output\n",
    "x = tf.placeholder(\"float\", [None, n_input, vocab_size])\n",
    "y = tf.placeholder(\"float\", [None, vocab_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = {\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden, vocab_size]))\n",
    "}\n",
    "biases = {\n",
    "    'out': tf.Variable(tf.random_normal([vocab_size]))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN Model\n",
    "\n",
    "A two-layered LSTM model is used.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward pass for the recurrent neural network\n",
    "def RNN(x, weights, biases):\n",
    "    x = tf.unstack(x, n_input, 1)\n",
    "    # 2 layered LSTM Definition\n",
    "    rnn_cell = rnn.MultiRNNCell([rnn.BasicLSTMCell(n_hidden),rnn.BasicLSTMCell(n_hidden)])\n",
    "    # generate prediction\n",
    "    outputs, states = rnn.static_rnn(rnn_cell, x, dtype=tf.float32)\n",
    "    # there are n_input outputs but\n",
    "    # we only want the last output\n",
    "    return tf.matmul(outputs[-1], weights['out']) + biases['out']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = RNN(x, weights, biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model evaluation\n",
    "correct_pred = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At each iteration, 3 sequence of input words and its subsequent word as the output are selected randomly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter= 500, Average Loss= 4.537830, Average Accuracy= 11.00%\n",
      "['people', 'knew', 'that'] - Actual word:[:] vs Predicted word:[:]\n",
      "Iter= 1000, Average Loss= 3.685317, Average Accuracy= 23.40%\n",
      "['she', 'did', 'not'] - Actual word:[like] vs Predicted word:[like]\n",
      "Iter= 1500, Average Loss= 0.781747, Average Accuracy= 77.00%\n",
      "['that', 'makes', 'them'] - Actual word:[bitter] vs Predicted word:[bitter]\n",
      "Iter= 2000, Average Loss= 0.844376, Average Accuracy= 82.60%\n",
      "['very', 'ugly', ';'] - Actual word:[and] vs Predicted word:[and]\n",
      "Iter= 2500, Average Loss= 1.751117, Average Accuracy= 61.00%\n",
      "['pleased', 'at', 'having'] - Actual word:[found] vs Predicted word:[found]\n",
      "Iter= 3000, Average Loss= 0.572721, Average Accuracy= 86.20%\n",
      "['keeping', 'so', 'close'] - Actual word:[to] vs Predicted word:[to]\n",
      "Iter= 3500, Average Loss= 0.602516, Average Accuracy= 85.40%\n",
      "['that', 'makes', 'people'] - Actual word:[hot-tempered] vs Predicted word:[forget]\n",
      "Iter= 4000, Average Loss= 0.537070, Average Accuracy= 88.00%\n",
      "[\"'\", 'and', 'she'] - Actual word:[squeezed] vs Predicted word:[squeezed]\n",
      "Iter= 4500, Average Loss= 0.262003, Average Accuracy= 94.60%\n",
      "['my', 'kitchen', 'at'] - Actual word:[all] vs Predicted word:[all]\n",
      "Iter= 5000, Average Loss= 0.150194, Average Accuracy= 97.00%\n",
      "['the', 'duchess', '.'] - Actual word:['] vs Predicted word:[']\n",
      "Iter= 5500, Average Loss= 0.365405, Average Accuracy= 94.20%\n",
      "['(', 'not', 'in'] - Actual word:[a] vs Predicted word:[a]\n",
      "Iter= 6000, Average Loss= 0.153624, Average Accuracy= 97.20%\n",
      "[',', 'child', '!'] - Actual word:['] vs Predicted word:[']\n",
      "Iter= 6500, Average Loss= 0.236418, Average Accuracy= 97.00%\n",
      "['when', \"i'm\", 'a'] - Actual word:[duchess] vs Predicted word:[duchess]\n",
      "Iter= 7000, Average Loss= 0.277153, Average Accuracy= 97.00%\n",
      "[\"'\", \"'\", 'perhaps'] - Actual word:[it] vs Predicted word:[it]\n",
      "Iter= 7500, Average Loss= 0.091065, Average Accuracy= 98.40%\n",
      "['when', 'they', 'met'] - Actual word:[in] vs Predicted word:[in]\n",
      "Iter= 8000, Average Loss= 0.126039, Average Accuracy= 98.40%\n",
      "['shall', 'remember', 'it'] - Actual word:[in] vs Predicted word:[in]\n",
      "Iter= 8500, Average Loss= 0.116961, Average Accuracy= 98.40%\n",
      "['a', 'pleasant', 'temper'] - Actual word:[,] vs Predicted word:[,]\n",
      "Iter= 9000, Average Loss= 0.075705, Average Accuracy= 99.00%\n",
      "['to', 'talk', '.'] - Actual word:[i] vs Predicted word:[i]\n",
      "Iter= 9500, Average Loss= 0.193065, Average Accuracy= 96.80%\n",
      "['very', 'glad', 'to'] - Actual word:[find] vs Predicted word:[find]\n",
      "Iter= 10000, Average Loss= 0.064222, Average Accuracy= 99.00%\n",
      "[\"'\", \"you're\", 'thinking'] - Actual word:[about] vs Predicted word:[about]\n",
      "Iter= 10500, Average Loss= 0.044824, Average Accuracy= 99.40%\n",
      "['the', 'duchess', ','] - Actual word:[as] vs Predicted word:[as]\n",
      "Iter= 11000, Average Loss= 0.042617, Average Accuracy= 99.40%\n",
      "[\"'she\", 'had', 'quite'] - Actual word:[forgotten] vs Predicted word:[forgotten]\n",
      "Iter= 11500, Average Loss= 0.108934, Average Accuracy= 98.60%\n",
      "['well', 'as', 'she'] - Actual word:[could] vs Predicted word:[could]\n",
      "Iter= 12000, Average Loss= 0.090571, Average Accuracy= 98.80%\n",
      "[\"wouldn't\", 'be', 'so'] - Actual word:[stingy] vs Predicted word:[stingy]\n",
      "Iter= 12500, Average Loss= 0.089831, Average Accuracy= 98.20%\n",
      "['she', 'bore', 'it'] - Actual word:[as] vs Predicted word:[as]\n",
      "Iter= 13000, Average Loss= 0.152981, Average Accuracy= 98.00%\n",
      "['people', 'knew', 'that'] - Actual word:[:] vs Predicted word:[:]\n",
      "Iter= 13500, Average Loss= 0.086919, Average Accuracy= 98.60%\n",
      "['.', 'however,', 'she'] - Actual word:[did] vs Predicted word:[did]\n",
      "Iter= 14000, Average Loss= 0.067649, Average Accuracy= 99.20%\n",
      "['things', 'that', 'make'] - Actual word:[children] vs Predicted word:[children]\n",
      "Iter= 14500, Average Loss= 0.126090, Average Accuracy= 98.40%\n",
      "[',', 'and', 'it'] - Actual word:[was] vs Predicted word:[was]\n",
      "Iter= 15000, Average Loss= 0.075209, Average Accuracy= 99.00%\n",
      "['sour', '\\xe2\\x80\\x94', 'and'] - Actual word:[camomile] vs Predicted word:[camomile]\n",
      "Iter= 15500, Average Loss= 0.087846, Average Accuracy= 99.00%\n",
      "['ugly', ';', 'and'] - Actual word:[secondly] vs Predicted word:[secondly]\n",
      "Iter= 16000, Average Loss= 0.056621, Average Accuracy= 99.20%\n",
      "['new', 'kind', 'of'] - Actual word:[rule] vs Predicted word:[rule]\n",
      "Iter= 16500, Average Loss= 0.143231, Average Accuracy= 98.20%\n",
      "[',', 'because', 'the'] - Actual word:[duchess] vs Predicted word:[duchess]\n",
      "Iter= 17000, Average Loss= 0.052740, Average Accuracy= 99.20%\n",
      "[',', 'very', 'much'] - Actual word:[pleased] vs Predicted word:[pleased]\n",
      "Iter= 17500, Average Loss= 0.076240, Average Accuracy= 99.00%\n",
      "['did', 'not', 'much'] - Actual word:[like] vs Predicted word:[like]\n",
      "Iter= 18000, Average Loss= 0.033750, Average Accuracy= 99.60%\n",
      "['.', 'soup', 'does'] - Actual word:[very] vs Predicted word:[very]\n",
      "Iter= 18500, Average Loss= 0.100022, Average Accuracy= 99.00%\n",
      "['only', 'you', 'can'] - Actual word:[find] vs Predicted word:[find]\n",
      "Iter= 19000, Average Loss= 0.165684, Average Accuracy= 98.40%\n",
      "['hopeful', 'tone', 'though'] - Actual word:[)] vs Predicted word:[)]\n",
      "Iter= 19500, Average Loss= 0.078828, Average Accuracy= 97.60%\n",
      "[\"'\", 'said', 'the'] - Actual word:[duchess] vs Predicted word:[duchess]\n",
      "Iter= 20000, Average Loss= 0.091578, Average Accuracy= 98.80%\n",
      "['.', \"'\", 'when'] - Actual word:[i'm] vs Predicted word:[i'm]\n",
      "Iter= 20500, Average Loss= 0.142925, Average Accuracy= 98.40%\n",
      "['it', \"hasn't\", 'one'] - Actual word:[,] vs Predicted word:[,]\n",
      "Iter= 21000, Average Loss= 0.130359, Average Accuracy= 98.20%\n",
      "['met', 'in', 'the'] - Actual word:[kitchen] vs Predicted word:[kitchen]\n",
      "Iter= 21500, Average Loss= 0.201476, Average Accuracy= 96.40%\n",
      "['a', 'bit', '.'] - Actual word:['] vs Predicted word:[']\n",
      "Iter= 22000, Average Loss= 0.065838, Average Accuracy= 98.80%\n",
      "['it', 'was', 'only'] - Actual word:[the] vs Predicted word:[the]\n",
      "Iter= 22500, Average Loss= 0.100790, Average Accuracy= 97.80%\n",
      "['the', 'moral', 'of'] - Actual word:[that] vs Predicted word:[that]\n",
      "Iter= 23000, Average Loss= 0.058750, Average Accuracy= 99.20%\n",
      "['pleasant', 'temper', ','] - Actual word:[and] vs Predicted word:[and]\n",
      "Iter= 23500, Average Loss= 0.110077, Average Accuracy= 98.80%\n",
      "[',', 'and', 'that'] - Actual word:[makes] vs Predicted word:[makes]\n",
      "Iter= 24000, Average Loss= 0.073569, Average Accuracy= 99.20%\n",
      "['and', 'they', 'walked'] - Actual word:[off] vs Predicted word:[off]\n",
      "Iter= 24500, Average Loss= 0.042132, Average Accuracy= 99.60%\n",
      "['startled', 'when', 'she'] - Actual word:[heard] vs Predicted word:[heard]\n",
      "Iter= 25000, Average Loss= 0.116889, Average Accuracy= 98.40%\n",
      "['old', 'thing', '!'] - Actual word:['] vs Predicted word:[']\n",
      "Iter= 25500, Average Loss= 0.026088, Average Accuracy= 99.20%\n",
      "['duchess', 'by', 'this'] - Actual word:[time] vs Predicted word:[time]\n",
      "Iter= 26000, Average Loss= 0.102994, Average Accuracy= 99.00%\n",
      "['you', 'again', ','] - Actual word:[you] vs Predicted word:[you]\n",
      "Iter= 26500, Average Loss= 0.103179, Average Accuracy= 98.00%\n",
      "['you', 'know', '\\xe2\\x80\\x94'] - Actual word:['she] vs Predicted word:['she]\n",
      "Iter= 27000, Average Loss= 0.046331, Average Accuracy= 99.20%\n",
      "['as', 'well', 'as'] - Actual word:[she] vs Predicted word:[she]\n",
      "Iter= 27500, Average Loss= 0.027637, Average Accuracy= 99.20%\n",
      "['then', 'they', \"wouldn't\"] - Actual word:[be] vs Predicted word:[be]\n",
      "Iter= 28000, Average Loss= 0.139183, Average Accuracy= 98.60%\n",
      "['uncomfortably', 'sharp', 'chin'] - Actual word:[.] vs Predicted word:[.]\n",
      "Iter= 28500, Average Loss= 0.075345, Average Accuracy= 99.20%\n",
      "['things', 'that', 'make'] - Actual word:[children] vs Predicted word:[children]\n",
      "Iter= 29000, Average Loss= 0.149943, Average Accuracy= 98.00%\n",
      "['an', 'uncomfortably', 'sharp'] - Actual word:[chin] vs Predicted word:[chin]\n",
      "Iter= 29500, Average Loss= 0.056143, Average Accuracy= 98.60%\n",
      "['that', 'makes', 'them'] - Actual word:[bitter] vs Predicted word:[sour]\n",
      "Iter= 30000, Average Loss= 0.058158, Average Accuracy= 99.00%\n",
      "['was', 'exactly', 'the'] - Actual word:[right] vs Predicted word:[right]\n",
      "Iter= 30500, Average Loss= 0.099139, Average Accuracy= 98.60%\n",
      "['of', 'rule', ','] - Actual word:['] vs Predicted word:[']\n",
      "Iter= 31000, Average Loss= 0.101596, Average Accuracy= 98.80%\n",
      "['very', 'ugly', ';'] - Actual word:[and] vs Predicted word:[and]\n",
      "Iter= 31500, Average Loss= 0.074418, Average Accuracy= 99.00%\n",
      "[',', 'very', 'much'] - Actual word:[pleased] vs Predicted word:[pleased]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter= 32000, Average Loss= 0.000043, Average Accuracy= 100.00%\n",
      "['side', 'as', 'she'] - Actual word:[spoke] vs Predicted word:[spoke]\n",
      "Iter= 32500, Average Loss= 0.138463, Average Accuracy= 98.60%\n",
      "['well', 'without', '\\xe2\\x80\\x94'] - Actual word:[maybe] vs Predicted word:[maybe]\n",
      "Iter= 33000, Average Loss= 0.082652, Average Accuracy= 98.60%\n",
      "[\"'\", 'and', 'she'] - Actual word:[squeezed] vs Predicted word:[squeezed]\n",
      "Iter= 33500, Average Loss= 0.100275, Average Accuracy= 97.80%\n",
      "['kitchen', 'at', 'all'] - Actual word:[.] vs Predicted word:[.]\n",
      "Iter= 34000, Average Loss= 0.059269, Average Accuracy= 98.40%\n",
      "['if', 'only', 'you'] - Actual word:[can] vs Predicted word:[can]\n",
      "Iter= 34500, Average Loss= 0.054746, Average Accuracy= 99.00%\n",
      "['not', 'in', 'a'] - Actual word:[very] vs Predicted word:[very]\n",
      "Iter= 35000, Average Loss= 0.060881, Average Accuracy= 99.40%\n",
      "['.', \"'\", 'tut'] - Actual word:[,] vs Predicted word:[,]\n",
      "Iter= 35500, Average Loss= 0.058720, Average Accuracy= 99.40%\n",
      "['the', 'kitchen', '.'] - Actual word:['] vs Predicted word:[']\n",
      "Iter= 36000, Average Loss= 0.059454, Average Accuracy= 99.40%\n",
      "['in', 'a', 'bit'] - Actual word:[.] vs Predicted word:[.]\n",
      "Iter= 36500, Average Loss= 0.093281, Average Accuracy= 98.60%\n",
      "['made', 'her', 'so'] - Actual word:[savage] vs Predicted word:[savage]\n",
      "Iter= 37000, Average Loss= 0.073840, Average Accuracy= 98.60%\n",
      "[',', 'but', 'i'] - Actual word:[shall] vs Predicted word:[shall]\n",
      "Iter= 37500, Average Loss= 0.002784, Average Accuracy= 99.80%\n",
      "['only', 'the', 'pepper'] - Actual word:[that] vs Predicted word:[that]\n",
      "Iter= 38000, Average Loss= 0.136947, Average Accuracy= 98.60%\n",
      "['the', 'moral', 'of'] - Actual word:[that] vs Predicted word:[that]\n",
      "Iter= 38500, Average Loss= 0.078129, Average Accuracy= 98.00%\n",
      "['thought', 'to', 'herself'] - Actual word:[that] vs Predicted word:[that]\n",
      "Iter= 39000, Average Loss= 0.078758, Average Accuracy= 99.00%\n",
      "['makes', 'you', 'forget'] - Actual word:[to] vs Predicted word:[to]\n",
      "Iter= 39500, Average Loss= 0.039464, Average Accuracy= 99.60%\n",
      "['and', 'they', 'walked'] - Actual word:[off] vs Predicted word:[off]\n",
      "Iter= 40000, Average Loss= 0.086877, Average Accuracy= 99.00%\n",
      "['her', 'ear.', \"'\"] - Actual word:[you're] vs Predicted word:[you're]\n",
      "Iter= 40500, Average Loss= 0.084623, Average Accuracy= 98.40%\n",
      "['she', 'tucked', 'her'] - Actual word:[arm] vs Predicted word:[arm]\n",
      "Iter= 41000, Average Loss= 0.127927, Average Accuracy= 98.40%\n",
      "['and', 'was', 'a'] - Actual word:[little] vs Predicted word:[little]\n",
      "Iter= 41500, Average Loss= 0.116210, Average Accuracy= 98.40%\n",
      "['you', 'again', ','] - Actual word:[you] vs Predicted word:[you]\n",
      "Iter= 42000, Average Loss= 0.059613, Average Accuracy= 98.80%\n",
      "['had', 'quite', 'forgotten'] - Actual word:[the] vs Predicted word:[the]\n",
      "Iter= 42500, Average Loss= 0.119563, Average Accuracy= 98.80%\n",
      "['you', \"can't\", 'think'] - Actual word:[how] vs Predicted word:[how]\n",
      "Iter= 43000, Average Loss= 0.066879, Average Accuracy= 98.60%\n",
      "['they', \"wouldn't\", 'be'] - Actual word:[so] vs Predicted word:[so]\n",
      "Iter= 43500, Average Loss= 0.031603, Average Accuracy= 99.20%\n",
      "['however,', 'she', 'did'] - Actual word:[not] vs Predicted word:[not]\n",
      "Iter= 44000, Average Loss= 0.079449, Average Accuracy= 98.60%\n",
      "['such', 'things', 'that'] - Actual word:[make] vs Predicted word:[make]\n",
      "Iter= 44500, Average Loss= 0.041110, Average Accuracy= 99.20%\n",
      "[',', 'and', 'it'] - Actual word:[was] vs Predicted word:[was]\n",
      "Iter= 45000, Average Loss= 0.037945, Average Accuracy= 99.40%\n",
      "['sour', '\\xe2\\x80\\x94', 'and'] - Actual word:[camomile] vs Predicted word:[camomile]\n",
      "Iter= 45500, Average Loss= 0.046033, Average Accuracy= 99.00%\n",
      "['very', 'ugly', ';'] - Actual word:[and] vs Predicted word:[and]\n",
      "Iter= 46000, Average Loss= 0.081660, Average Accuracy= 98.80%\n",
      "['she', 'went', 'on'] - Actual word:[,] vs Predicted word:[,]\n",
      "Iter= 46500, Average Loss= 0.106180, Average Accuracy= 98.80%\n",
      "['not', 'much', 'like'] - Actual word:[keeping] vs Predicted word:[keeping]\n",
      "Iter= 47000, Average Loss= 0.236451, Average Accuracy= 96.60%\n",
      "['pepper', 'that', 'makes'] - Actual word:[people] vs Predicted word:[people]\n",
      "Iter= 47500, Average Loss= 0.064624, Average Accuracy= 98.80%\n",
      "[\"alice's\", 'side', 'as'] - Actual word:[she] vs Predicted word:[she]\n",
      "Iter= 48000, Average Loss= 0.085959, Average Accuracy= 99.20%\n",
      "['at', 'all', '.'] - Actual word:[soup] vs Predicted word:[soup]\n",
      "Iter= 48500, Average Loss= 0.041403, Average Accuracy= 99.00%\n",
      "['if', 'only', 'you'] - Actual word:[can] vs Predicted word:[can]\n",
      "Iter= 49000, Average Loss= 0.119834, Average Accuracy= 98.20%\n",
      "[',', \"'\", 'i'] - Actual word:[won't] vs Predicted word:[won't]\n",
      "Iter= 49500, Average Loss= 0.090761, Average Accuracy= 98.80%\n",
      "['said', 'the', 'duchess'] - Actual word:[.] vs Predicted word:[.]\n",
      "Iter= 50000, Average Loss= 0.058724, Average Accuracy= 99.40%\n",
      "['herself', ',', '('] - Actual word:[not] vs Predicted word:[not]\n",
      "TrainingCompleted!\n",
      "Complete sentence follows!\n",
      "i only wish hopeful tone though ) , ' i won't have any pepper in my kitchen at all . soup does very well without â€” maybe it's always pepper that\n"
     ]
    }
   ],
   "source": [
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "# Launch the graph\n",
    "with tf.Session() as session:\n",
    "    session.run(init)\n",
    "    step = 0\n",
    "    offset = random.randint(0,n_input+1)\n",
    "    end_offset = n_input + 1\n",
    "    acc_total = 0\n",
    "    loss_total = 0\n",
    "    \n",
    "    while step < training_iters:\n",
    "        if offset > (len(train_data)-end_offset):\n",
    "            offset = random.randint(0, n_input+1)\n",
    "            \n",
    "        symbols_in_keys = [ input_one_hot(dictionary[ str(train_data[i])]) for i in range(offset, offset+n_input) ]\n",
    "        symbols_in_keys = np.reshape(np.array(symbols_in_keys), [-1, n_input,vocab_size])\n",
    "        symbols_out_onehot = np.zeros([vocab_size], dtype=float)\n",
    "        symbols_out_onehot[dictionary[str(train_data[offset+n_input])]] = 1.0\n",
    "        symbols_out_onehot = np.reshape(symbols_out_onehot,[1,-1])\n",
    "        \n",
    "        _, acc, loss, onehot_pred = session.run([optimizer, accuracy, cost, pred], feed_dict={x: symbols_in_keys, y: symbols_out_onehot})\n",
    "        \n",
    "        loss_total += loss\n",
    "        acc_total += acc\n",
    "        \n",
    "        if (step+1) % display_step == 0:\n",
    "            print(\"Iter= \" + str(step+1) + \", Average Loss= \" + \\\n",
    "                  \"{:.6f}\".format(loss_total/display_step) + \", Average Accuracy= \" + \\\n",
    "                  \"{:.2f}%\".format(100*acc_total/display_step))\n",
    "            acc_total = 0\n",
    "            loss_total = 0\n",
    "            symbols_in = [train_data[i] for i in range(offset, offset + n_input)]\n",
    "            symbols_out = train_data[offset + n_input]\n",
    "            symbols_out_pred = reverse_dictionary[int(tf.argmax(onehot_pred, 1).eval())]\n",
    "            \n",
    "            print(\"%s - Actual word:[%s] vs Predicted word:[%s]\" % (symbols_in,symbols_out,symbols_out_pred))\n",
    "            \n",
    "        step += 1\n",
    "        offset += (n_input+1)\n",
    "    print(\"TrainingCompleted!\")\n",
    "    \n",
    "# Feed a 3-word sentence and let the model predict the next 28 words\n",
    "    sentence = 'i only wish'\n",
    "    words = sentence.split(' ')\n",
    "    try:\n",
    "        symbols_in_keys = [ input_one_hot(dictionary[ str(train_data[i])]) for i in\n",
    "        range(offset, offset+n_input) ]\n",
    "        for i in range(28):\n",
    "            keys = np.reshape(np.array(symbols_in_keys), [-1, n_input,vocab_size])\n",
    "            onehot_pred = session.run(pred, feed_dict={x: keys})\n",
    "            onehot_pred_index = int(tf.argmax(onehot_pred, 1).eval())\n",
    "            sentence = \"%s %s\" % (sentence,reverse_dictionary[onehot_pred_index])\n",
    "            symbols_in_keys = symbols_in_keys[1:]\n",
    "            symbols_in_keys.append(input_one_hot(onehot_pred_index))\n",
    "        print \"Complete sentence follows!\"\n",
    "        print(sentence)\n",
    "    except:\n",
    "        print(\"Error while processing the sentence to be completed\")\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
